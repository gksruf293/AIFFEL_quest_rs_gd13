{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617541ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu111\n",
      "1.21.4\n",
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# print(tensorflow.__version__)\n",
    "print(np.__version__)\n",
    "print(transformers.__version__)\n",
    "# print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5ccfb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu111\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953e74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.0\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (4.62.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (0.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (2.26.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (3.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (1.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.37.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2.0.8)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.3\n",
      "    Uninstalling transformers-4.52.3:\n",
      "      Successfully uninstalled transformers-4.52.3\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.37.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a403fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9efe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bef04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch transformers -y\n",
    "# !pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c4e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.40.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9d3b9",
   "metadata": {},
   "source": [
    "# NSMC 데이터 분석 및 huggingface dataset 구성\n",
    "- Naver Snetiment Movie Corpus task\n",
    "- klue/ber-base를 활용하여 NSMC task를 하는 것이 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0fd7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 149995\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 49997\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "# pandas로 데이터 읽기\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# 결측값 제거\n",
    "train_data = train_data.dropna(how='any').reset_index(drop=True)\n",
    "test_data = test_data.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb21a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 dict 형식으로 변환\n",
    "train_dict = train_dataset.to_dict('list')\n",
    "test_dict = test_dataset.to_dict('list')\n",
    "\n",
    "# dict로 변환하여 Hugging Face Dataset으로 다시 변환\n",
    "hf_train_dataset = Dataset.from_dict(train_dict)\n",
    "hf_test_dataset = Dataset.from_dict(test_dict)\n",
    "\n",
    "# train/validation split (임의로 20% validation 데이터로 사용)\n",
    "train_val_dataset = hf_train_dataset.train_test_split(test_size=0.2)\n",
    "tf_train_dataset = train_val_dataset['train']\n",
    "tf_val_dataset = train_val_dataset['test']\n",
    "tf_test_dataset = hf_test_dataset  # 테스트 데이터\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee24747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer와 model\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a1795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d954de5",
   "metadata": {},
   "source": [
    "# 불러온 tokenizer로 데이터셋 전처리, model 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # 토크나이저 로드\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # 문서 리스트\n",
    "#         truncation=True,  # 길이가 max_seq_len을 넘으면 자르기\n",
    "#         padding='max_length',  # 길이가 max_seq_len 미만이면 패딩\n",
    "#         max_length=max_seq_len,  # 최대 길이 설정\n",
    "#         return_tensors=None,  # 텐서를 반환하지 않음, 대신 리스트나 numpy 배열 반환\n",
    "#         return_attention_mask=True,  # Attention Mask 반환\n",
    "#         return_token_type_ids=False  # 단일 문장 분류이므로 token_type_ids 필요 없음\n",
    "#     )\n",
    "    \n",
    "#     # 토크나이즈된 값들\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "    \n",
    "#     # 레이블을 리스트로 변환\n",
    "#     data_labels = labels  # 이미 리스트 형태로 존재\n",
    "\n",
    "#     # 딕셔너리 형태로 반환, 텐서 대신 리스트로 반환\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_masks,\n",
    "#         'labels': data_labels\n",
    "#     }\n",
    "\n",
    "# # 예시 데이터\n",
    "# train_data = [\"이 영화 진짜 재밌어요\", \"영화 내용이 지루했어요\"]  # 예시 문장\n",
    "# train_labels = [1, 0]  # 예시 레이블: 1 = 긍정, 0 = 부정\n",
    "\n",
    "# # 데이터셋 변환\n",
    "# max_seq_len = 128  # 최대 시퀀스 길이 설정\n",
    "# train_dataset = Dataset.from_dict({'document': train_data, 'label': train_labels})\n",
    "\n",
    "# # 데이터셋에 전처리 적용\n",
    "# train_dataset = train_dataset.map(lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer), batched=True)\n",
    "\n",
    "# # 전처리된 데이터셋 확인\n",
    "# print(train_dataset[0])  # 첫 번째 샘플을 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6492f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b375d321524fb697fea658d4cf4a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efdeeae057744fda6e8b0a9ab257823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60081973674448b99bf6c1c15e78d505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9822864, 'document': '대서사시의 완벽하고 화려한 막', 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 18979, 2063, 2067, 2079, 5537, 19521, 5725, 2470, 1037, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1, 'label': 1}\n",
      "torch.Size([119996, 128]) torch.Size([119996, 128]) torch.Size([119996])\n",
      "torch.Size([29999, 128]) torch.Size([29999, 128]) torch.Size([29999])\n",
      "torch.Size([49997, 128]) torch.Size([49997, 128]) torch.Size([49997])\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 예시 데이터\n",
    "train_data = [\"이 영화 진짜 재밌어요\", \"영화 내용이 지루했어요\"]  # 예시 문장\n",
    "train_labels = [1, 0]  # 예시 레이블: 1 = 긍정, 0 = 부정\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    # 토크나이저를 사용하여 데이터 배치 처리\n",
    "    encodings = tokenizer(\n",
    "        examples,  # 문서 리스트\n",
    "        truncation=True,  # 길이가 max_seq_len을 넘으면 자르기\n",
    "        padding='max_length',  # 길이가 max_seq_len 미만이면 패딩\n",
    "        max_length=max_seq_len,  # 최대 길이 설정\n",
    "        return_tensors=None,  # 텐서를 반환하지 않고, 리스트나 numpy 배열로 반환\n",
    "        return_attention_mask=True,  # Attention Mask 반환\n",
    "        return_token_type_ids=False  # 단일 문장 분류이므로 token_type_ids 필요 없음\n",
    "    )\n",
    "    \n",
    "    # tokenized된 값들\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_masks = encodings['attention_mask']\n",
    "    \n",
    "    # 레이블을 텐서로 변환\n",
    "    data_labels = labels  # 이미 리스트 형태로 존재\n",
    "\n",
    "    # 딕셔너리 형태로 반환\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': data_labels\n",
    "    }\n",
    "\n",
    "# 데이터셋 전처리\n",
    "max_seq_len = 128  # 최대 시퀀스 길이 설정\n",
    "\n",
    "# tf_train_dataset, tf_val_dataset, tf_test_dataset에 전처리 적용\n",
    "tf_train_dataset = tf_train_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = tf_val_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tf_test_dataset = tf_test_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# 전처리된 데이터셋 확인 (첫 번째 샘플 확인)\n",
    "print(tf_train_dataset[0])  # 첫 번째 샘플을 확인\n",
    "\n",
    "# 텐서로 변환\n",
    "train_input_ids = torch.tensor(tf_train_dataset['input_ids'])\n",
    "train_attention_mask = torch.tensor(tf_train_dataset['attention_mask'])\n",
    "train_labels = torch.tensor(tf_train_dataset['labels'])\n",
    "\n",
    "val_input_ids = torch.tensor(tf_val_dataset['input_ids'])\n",
    "val_attention_mask = torch.tensor(tf_val_dataset['attention_mask'])\n",
    "val_labels = torch.tensor(tf_val_dataset['labels'])\n",
    "\n",
    "test_input_ids = torch.tensor(tf_test_dataset['input_ids'])\n",
    "test_attention_mask = torch.tensor(tf_test_dataset['attention_mask'])\n",
    "test_labels = torch.tensor(tf_test_dataset['labels'])\n",
    "\n",
    "# 텐서 형태로 잘 변환되었는지 확인\n",
    "print(train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "print(val_input_ids.shape, val_attention_mask.shape, val_labels.shape)\n",
    "print(test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3faedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "# train_data = pd.read_table('ratings_train.txt')\n",
    "# test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# train_data = train_data.dropna(how = 'any')\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "# print(train_data.isnull().values.any())\n",
    "\n",
    "# test_data = test_data.dropna(how = 'any')\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "# print(test_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00012cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b051e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# def convert_examples_to_features(examples,labels,max_seq_len,tokenizer):\n",
    "\n",
    "#   input_ids = []\n",
    "#   attention_masks = []\n",
    "#   token_type_ids = []\n",
    "#   data_labels = []\n",
    "\n",
    "#   for example, label in tqdm(zip(examples,labels),total=len(examples)):\n",
    "\n",
    "#     input_id = tokenizer.encode(example,max_length=max_seq_len,pad_to_max_length=True)\n",
    "#     padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "#     attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "#     token_type_id = [0] * max_seq_len\n",
    "\n",
    "#     assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "#     assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "#     assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "#     input_ids.append(input_id)\n",
    "#     attention_masks.append(attention_mask)\n",
    "#     token_type_ids.append(token_type_id)\n",
    "#     data_labels.append(label)\n",
    "\n",
    "#   input_ids = torch.tensor(input_ids)\n",
    "#   attention_masks = torch.tensor(attention_masks)\n",
    "#   token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "#   data_labels = torch.tensor(data_labels)\n",
    "\n",
    "#   return (input_ids, attention_masks, token_type_ids), data_labels\n",
    "\n",
    "# max_seq_len = 128\n",
    "\n",
    "# train_X,train_y = convert_examples_to_features(tf_train_dataset[\"document\"],tf_train_dataset[\"label\"],max_seq_len = max_seq_len, tokenizer = tokenizer)\n",
    "# val_X, val_y = convert_examples_to_features(tf_val_dataset[\"document\"],tf_val_dataset[\"label\"],max_seq_len = max_seq_len,tokenizer=tokenizer)\n",
    "# test_X, test_y = convert_examples_to_features(tf_test_dataset[\"document\"],tf_test_dataset[\"label\"],max_seq_len = max_seq_len,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     data_labels = []\n",
    "\n",
    "#     # 토크나이저를 사용하여 데이터 배치 처리\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # 문서 리스트\n",
    "#         truncation=True,  # 길이가 max_seq_len을 넘으면 자르기\n",
    "#         padding='max_length',  # 길이가 max_seq_len 미만이면 패딩\n",
    "#         max_length=max_seq_len,  # 최대 길이 설정\n",
    "#         return_tensors='pt',  # 텐서로 반환 (PyTorch)\n",
    "#         return_attention_mask=True,  # Attention Mask 반환\n",
    "#         return_token_type_ids=False  # 단일 문장 분류이므로 token_type_ids 필요 없음\n",
    "#     )\n",
    "\n",
    "#     # input_ids, attention_mask, labels를 리스트에 추가\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "#     data_labels = torch.tensor(labels)  # 레이블을 텐서로 변환\n",
    "\n",
    "#     return (input_ids, attention_masks), data_labels\n",
    "\n",
    "# # 최대 시퀀스 길이 설정\n",
    "# max_seq_len = 128\n",
    "\n",
    "# # 전처리 실행\n",
    "# train_X, train_y = convert_examples_to_features(\n",
    "#     tf_train_dataset[\"document\"], \n",
    "#     tf_train_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len, \n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n",
    "# val_X, val_y = convert_examples_to_features(\n",
    "#     tf_val_dataset[\"document\"], \n",
    "#     tf_val_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len,\n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n",
    "# test_X, test_y = convert_examples_to_features(\n",
    "#     tf_test_dataset[\"document\"], \n",
    "#     tf_test_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len, \n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc10c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     # 입력값을 담을 리스트 초기화\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     data_labels = []\n",
    "    \n",
    "#     # 토크나이저를 사용하여 데이터 배치 처리\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # 문서 리스트\n",
    "#         truncation=True,  # 길이가 max_seq_len을 넘으면 자르기\n",
    "#         padding='max_length',  # 길이가 max_seq_len 미만이면 패딩\n",
    "#         max_length=max_seq_len,  # 최대 길이 설정\n",
    "#         return_tensors='pt',  # 텐서로 반환 (PyTorch)\n",
    "#         return_attention_mask=True,  # Attention Mask 반환\n",
    "#         return_token_type_ids=False  # 단일 문장 분류이므로 token_type_ids 필요 없음\n",
    "#     )\n",
    "    \n",
    "#     # tokenized된 값들\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "    \n",
    "#     # 레이블을 텐서로 변환\n",
    "#     data_labels = torch.tensor(labels)  # 레이블을 텐서로 변환\n",
    "    \n",
    "#     # 딕셔너리 형태로 반환\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_masks,\n",
    "#         'labels': data_labels\n",
    "#     }\n",
    "\n",
    "# # 예시 데이터\n",
    "# max_seq_len = 128  # 최대 시퀀스 길이\n",
    "# train_data = [\"이 영화 진짜 재밌어요\", \"영화 내용이 지루했어요\"]  # 예시 문장\n",
    "# train_labels = [1, 0]  # 예시 레이블: 1 = 긍정, 0 = 부정\n",
    "\n",
    "# # 토크나이저 로드\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# # 전처리 함수 호출\n",
    "# features = convert_examples_to_features(train_data, train_labels, max_seq_len, tokenizer)\n",
    "\n",
    "# # 전처리된 데이터 출력\n",
    "# print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93783c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U\n",
    "# !pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aec2767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.40.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.1\n",
      "    Uninstalling transformers-4.40.1:\n",
      "      Successfully uninstalled transformers-4.40.1\n",
      "Successfully installed tokenizers-0.21.1 transformers-4.52.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "output_dir = './transformers2'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1bdafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric_accuracy = load_metric(\"accuracy\")\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)  # 예측값을 라벨로 변환 (가장 높은 확률의 클래스 선택)\n",
    "    \n",
    "    # 정확도와 F1-score 계산\n",
    "    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5bd3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7122f280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 3:42:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.286232</td>\n",
       "      <td>0.897997</td>\n",
       "      <td>0.897563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.374714</td>\n",
       "      <td>0.904863</td>\n",
       "      <td>0.903854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.458725</td>\n",
       "      <td>0.903263</td>\n",
       "      <td>0.903150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=tf_train_dataset,    # training dataset\n",
    "    eval_dataset=tf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ca05",
   "metadata": {},
   "source": [
    "- accuracy :90%이상\n",
    "- f1-score 90% 이상\n",
    "- training loss는 꾸준히 감소하고 있다. 0.2940 -> 0.1630\n",
    "- validation loss는 증가. 0.2862 -> 0.4487\n",
    "- Accuracy / F1 Score는 약간 증가 후 약간 하락\n",
    "\n",
    "    Accuracy:\n",
    "\n",
    "        Epoch 1 → 2: 상승 (0.8979 → 0.9048)\n",
    "\n",
    "        Epoch 2 → 3: 하락 (0.9048 → 0.9032)\n",
    "\n",
    "    F1 Score:\n",
    "\n",
    "        Epoch 1 → 2: 상승 (0.8975 → 0.9038)\n",
    "\n",
    "        Epoch 2 → 3: 하락 (0.9038 → 0.9031)\n",
    "\n",
    "→ 성능이 정체된 상태이며, 과적합 의심\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe9c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 마지막 저장된 체크포인트를 로드 (예: checkpoint-5000)\n",
    "# model = BertForSequenceClassification.from_pretrained('./transformers/checkpoint-5000')\n",
    "\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "trainer.save_model('./basemodel2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b627fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 08:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46605202555656433,\n",
       " 'eval_accuracy': 0.9026541592495549,\n",
       " 'eval_f1': 0.9044017992182436,\n",
       " 'eval_runtime': 508.4451,\n",
       " 'eval_samples_per_second': 98.333,\n",
       " 'eval_steps_per_second': 12.292,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb8066",
   "metadata": {},
   "source": [
    "- Accuracy/F1 90% 이상으로 우수\n",
    "- eval loss 0.46으로 약간 높은 편, 약간의 overfitting\n",
    "- validation과 test에서 모두 우수한 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99506b02",
   "metadata": {},
   "source": [
    "# fine-tuning을 통하여 모델 accuracy 향상시키기\n",
    "- early stopping 적용\n",
    "- learning rate scheduler 사용\n",
    "- learning rate 감소\n",
    "- dropout 조정\n",
    "- mixed precision traing  fp16 = true로 학습속도 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4ab7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "#메모리 초기화\n",
    "import torch, gc\n",
    "\n",
    "def reset_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✅ GPU memory cleared\")\n",
    "\n",
    "reset_cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c565464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88992926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('klue/bert-base', num_labels=2)\n",
    "config.hidden_dropout_prob = 0.3  # 기본은 0.1 → 과적합 방지를 위해 증가\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'klue/bert-base', \n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35125' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35125/75000 1:45:00 < 1:59:12, 5.57 it/s, Epoch 4.68/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.276043</td>\n",
       "      <td>0.889363</td>\n",
       "      <td>0.888874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.283571</td>\n",
       "      <td>0.897463</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.261236</td>\n",
       "      <td>0.904430</td>\n",
       "      <td>0.903295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.331682</td>\n",
       "      <td>0.902430</td>\n",
       "      <td>0.903415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size =16,   # 각 device 당 batch size 8 -> 16로 \n",
    "    per_device_eval_batch_size = 16,  \n",
    "    learning_rate=1e-5,  \n",
    "    lr_scheduler_type = 'cosine',                       #cosine scheduler 추가\n",
    "    warmup_ratio = 0.1,                            #전체 step의 10%\n",
    "    num_train_epochs=10,                          # 💡 EarlyStopping이 있으므로 더 크게 설정 가능\n",
    "    weight_decay=0.01,                           # 일반화 성능 향상 l2 legularization\n",
    "    load_best_model_at_end=True,                 # ✅ EarlyStopping과 함께 사용\n",
    "    metric_for_best_model=\"accuracy\",           # 조기 종료 기준 메트릭\n",
    "    greater_is_better=True,                     # 높을수록 좋음 (accuracy의 경우)\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True     #학습 속도 높이기, 메모리 사용 줄이기\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tf_train_dataset,\n",
    "    eval_dataset=tf_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # ✅ patience: 2 epoch\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./fine_tuned_model')\n",
    "trainer.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc51ba",
   "metadata": {},
   "source": [
    "- train/val loss가 안정적으로 줄어든다. \n",
    "- accuracy/f1이 90% 이상으로 우수\n",
    "- epoch 3 이후 accuracy가 떨어지고 train/val loss 증가\n",
    "- epoch 수는 3을 넘지 않는 것이 좋아보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc364cd5",
   "metadata": {},
   "source": [
    "# Bucketing을 적용하여 학습시키고 이전 모델(basemodel)과 비교\n",
    "- 패딩을 dynamic으로 바꾼다음에 bucketing 적용\n",
    "- 텍스트 길이에 따라 여러개의 bucket으로 나눠서 비슷한 길이끼리 같은 batch 구성\n",
    "- 모델 성능 향상 측면\n",
    "- 훈련 시간 측면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f232d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "#메모리 초기화\n",
    "import torch, gc\n",
    "\n",
    "def reset_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✅ GPU memory cleared\")\n",
    "\n",
    "reset_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e156cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# 데이터 다운로드\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", \"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", \"ratings_test.txt\")\n",
    "\n",
    "# pandas 로드 및 전처리\n",
    "train_df = pd.read_table(\"ratings_train.txt\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_table(\"ratings_test.txt\").dropna().reset_index(drop=True)\n",
    "\n",
    "# HuggingFace Dataset 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9266dfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743bbdc7a054b3ea5de7ba83bba8bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be775e8c4dba407dbd933e7fffbbf9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dynamic padding 적용\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# tokenizer 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # ✅ dynamic padding을 위해 False\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "# tokenize 적용\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6a6ad7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c50ea26dac24a49937202ff467a259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149995 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aac3d28ab743128bcfa48c0b193735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49997 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_length(example):\n",
    "    example[\"length\"] = sum(example[\"attention_mask\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(add_length)\n",
    "test_dataset = test_dataset.map(add_length)\n",
    "\n",
    "# 정렬\n",
    "train_dataset = train_dataset.sort(\"length\")\n",
    "test_dataset = test_dataset.sort(\"length\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0500d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e027a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sorted_indices = np.argsort(lengths)\n",
    "        self.batches = [\n",
    "            self.sorted_indices[i:i + batch_size]\n",
    "            for i in range(0, len(self.sorted_indices), batch_size)\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.batches)\n",
    "        for batch in self.batches:\n",
    "            yield batch.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0f96fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.dataset[idx][\"input_ids\"],           # ✅ 리스트 그대로 반환\n",
    "            \"attention_mask\": self.dataset[idx][\"attention_mask\"],\n",
    "            \"labels\": self.dataset[idx][\"label\"],                  # ❗ 'label' → 'labels'로 이름만 통일\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a2b5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "# test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e5436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Dataset 생성\n",
    "train_data = CustomDataset(train_dataset)\n",
    "test_data = CustomDataset(test_dataset)\n",
    "\n",
    "# BucketBatchSampler\n",
    "train_sampler = BucketBatchSampler(train_dataset[\"length\"], batch_size=16)\n",
    "test_sampler = BucketBatchSampler(test_dataset[\"length\"], batch_size=16)\n",
    "\n",
    "# Collator: 동적 패딩 적용\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_sampler=train_sampler, collate_fn=collator)\n",
    "test_loader = DataLoader(test_data, batch_sampler=test_sampler, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "db4fd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e125663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c18d1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33f245c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Epoch 1 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.2739, Val Loss: 0.2474, Acc: 0.9002, F1: 0.9030\n",
      "✅ Best model saved (epoch 1)\n",
      "\n",
      "🔁 Epoch 2 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.1844, Val Loss: 0.2446, Acc: 0.9044, F1: 0.9036\n",
      "✅ Best model saved (epoch 2)\n",
      "\n",
      "🔁 Epoch 3 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.1215, Val Loss: 0.2773, Acc: 0.8995, F1: 0.9033\n",
      "\n",
      "🕒 총 학습 시간: 0시간 58분 48초\n",
      "\n",
      "📊 [Test] Evaluating best model...\n",
      "[Test] Loss: 0.2446 | Acc: 0.9044 | F1: 0.9036\n",
      "🕒 Test Evaluation Time: 89.72 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2).to(device)\n",
    "\n",
    "# ✅ Optimizer 설정 (Trainer와 동일하게 설정)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 평가 지표 함수\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')  # binary classification\n",
    "    return total_loss / len(data_loader), acc, f1\n",
    "\n",
    "# ✅ 훈련 함수\n",
    "def train_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# ✅ 전체 학습 루프\n",
    "best_f1 = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"\\n🔁 Epoch {epoch+1} 시작\")\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, test_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # ✅ 가장 좋은 모델 저장\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_bucketing_model.pt\")\n",
    "        print(f\"✅ Best model saved (epoch {epoch+1})\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# ✅ 학습 시간 출력\n",
    "elapsed = end_time - start_time\n",
    "hours, minutes, seconds = int(elapsed // 3600), int((elapsed % 3600) // 60), int(elapsed % 60)\n",
    "print(f\"\\n🕒 총 학습 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
    "\n",
    "# ✅ 테스트 평가\n",
    "print(\"\\n📊 [Test] Evaluating best model...\")\n",
    "test_start_time = time.time()\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_bucketing_model.pt\"))\n",
    "test_loss, test_acc, test_f1 = evaluate(model, test_loader)\n",
    "\n",
    "test_end_time = time.time()\n",
    "test_duration = test_end_time - test_start_time\n",
    "\n",
    "print(f\"[Test] Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f}\")\n",
    "print(f\"🕒 Test Evaluation Time: {test_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c4eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4caf9cee",
   "metadata": {},
   "source": [
    "## 📊 모델 성능 비교\n",
    "\n",
    "| 항목               | 이전 모델 (Trainer) | 현재 모델 (Custom loop + Bucketing) |\n",
    "|--------------------|----------------------|--------------------------------------|\n",
    "| **Test Loss**      | 0.4660               | 0.2446                               |\n",
    "| **Test Accuracy**  | 0.8601               | 0.9044                               |\n",
    "| **Test F1 Score**  | 0.8540               | 0.9036                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72c38b",
   "metadata": {},
   "source": [
    "## 훈련 시간 비교\n",
    "| 항목                | 이전 모델 (Trainer) | 현재 모델 (Custom loop + Bucketing)     |\n",
    "| ----------------- | --------------- | ----------------------------------- |\n",
    "| **총 Epoch 수**     | 3            | 3| \n",
    "| **총 훈련 시간**       | 3시간 42분      |    58분 48초                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9eb0e",
   "metadata": {},
   "source": [
    "bucketing의 효과\n",
    "- 모델 성능을 비교하였을 때 이전 모델보다 성능이 개선됨(accuracy, f1)\n",
    "- 시간효율적 측면에서 훈련시간이 3시간 42분에서 58분 48초로 75%감소\n",
    "- 동적패딩 + bucketing 전략을 사용하면 훈련속도와 모델 성능 모두에 긍정적인 영향을 준다고 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261bf065",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 학습 도중 OOM(Out of Memory) 에러가 빈번하게 발생했으나, 모델 학습 및 저장 후 GPU메모리를 명시적으로 초기화함으로써 문제를 해결할 수 있었다.\n",
    "- 메모리 최적화 루틴(gc.collect() + torch.cuda.empty_cache())의 중요성을 경험했다. \n",
    "- basemodel 학습 시 validation loss가 점차 증가하며 과적합 양상을 보였다.\n",
    "- dropout 적용, learning rate 조정, early stopping 등을 적용하여 validation loss가 감소하는 것을 확인하였지만 accuracy는 크게 향상되지 않음(0.9026 -> 0.9032)\n",
    "- epoch은 3을 넘지 않아야 과적합을 방지할 수 있었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615abc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
