# AIFFEL Campus Online Code Peer Review Templete
- 코더 : 서지연
- 리뷰어 : 이현재


# PRT(Peer Review Template)
- [x]  **1. 주어진 문제를 해결하는 완성된 코드가 제출되었나요?**
    - Example에 대한 예측과 Bleu score를 출력했다. Bleu score가 41점으로 높았다.
        - ![image](https://github.com/user-attachments/assets/163e7d1a-50de-4521-96b8-84f33d8476bf)

    
- [ ]  **2. 전체 코드에서 가장 핵심적이거나 가장 복잡하고 이해하기 어려운 부분에 작성된 
주석 또는 doc string을 보고 해당 코드가 잘 이해되었나요?**
    - 해당 코드 블럭을 왜 핵심적이라고 생각하는지 확인
    - 해당 코드 블럭에 doc string/annotation이 달려 있는지 확인
    - 해당 코드의 기능, 존재 이유, 작동 원리 등을 기술했는지 확인
    - 주석을 보고 코드 이해가 잘 되었는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
        
- [ ]  **3. 에러가 난 부분을 디버깅하여 문제를 해결한 기록을 남겼거나
새로운 시도 또는 추가 실험을 수행해봤나요?**
    - 문제 원인 및 해결 과정을 잘 기록하였는지 확인
    - 프로젝트 평가 기준에 더해 추가적으로 수행한 나만의 시도, 
    실험이 기록되어 있는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
        
- [x]  **4. 회고를 잘 작성했나요?**
    - 결과정리와 회고를 잘 작성해주셨다.
        - ![image](https://github.com/user-attachments/assets/07ecfba5-166b-4376-81e7-997fca334a8b)

        
- [ ]  **5. 코드가 간결하고 효율적인가요?**
    - 파이썬 스타일 가이드 (PEP8) 를 준수하였는지 확인
    - 코드 중복을 최소화하고 범용적으로 사용할 수 있도록 함수화/모듈화했는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부


# 회고(참고 링크 및 코드 개선)
```
번역기 프로젝트에서는 데이터 의미 대응 문제로 시도해보지 못했던 Lexical substitution을 시도한 부분이 인상깊었다.
모델 타입을 unigram으로 했을 때와 bpe로 했을 때 답변 품질이 눈에 띄게 차이가 나서 흥미로웠다. 아마 bpe의 vocab_size가 unigram보다 훨씬 커서 학습이 잘 된 듯하다.

```

